This is the result of me following along with the following articles:

[The attention mechanism](https://benjaminwarner.dev/2023/07/01/attention-mechanism)
[The rest of the transformer](https://benjaminwarner.dev/2023/07/28/rest-of-the-transformer)
[His source code repo](https://github.com/warner-benjamin/commented-transformers)

Most comments are just my re-writing things stated in the articles.
I do not claim to own Benjamin Warner's work, I'm simply rewriting some bits of his articles while following along with the tutorial.
