# import math

# import torch
import torch.nn as nn
# from torch import Tensor, BoolTensor
# from torch.nn import functional as F

"""
Overview of attention mechanism
out: Query Q, Key K and Value V matrices
in: X via linear layers with learnable weights W^Q, W^K and W^V

Q=XW^Q, K=XW^K, V=XW^V

where W^Q is in R^{dim_model x dim_Q}
less formally, Q=XW^{Q} is a set of linear equations
Q=XA^Q + B^Q where A^Q and B^Q are learnable params for calculating Q from X

Attention is calculated by:
Att(Q,K,V) = softmax(QK^T/sqrt(d_k))V

where the denom is a scaling factor usually based on head dimension or num heads
For most implementations, Q,K,V all have the same shape

usually then passed trhough a linear layer W^O projection:
Output = Att(Q,K,V)W^O

essentially a learned weighted avg
attn learns to generate weights btwn tokens via queries and keys
per-token weights generated by softmax
values XW^V learn to create a token repr
which can incorp weighted avg of all other tokens in final dot prod of layer
when token attends to another, its increasing size of second's weight in avg relative to all other tokens
"""

"""
Flavors of attention:
Bidirectional | fully visible
Causal | Autoregressive
Cross

first 2: self-attention, only apply to one input seq
cross applies on multiple inputs

Bi is used in enc blocks in encoder-only models (BERT) or enc-dec models (BART)
allows attn mech to incorp both prior and successive tokens, regardless of order
used when we want context from entire input (classification)

causal: decoder blocks in dec-only (GPT) or enc-dec (BART)
only can work with info from prior tokens
used when trying to preserve temporal structure, like next token generation

cross attn in cross blocks in enc-dec
allows inc a diff seq of tokens to current seq
used when aligning 2 seq: translating from lang/domain to another
or multiple input types, like text and img in diffusion models
"""

"""
Single head self-attention
Simplest mechanism:
Single Head Bidirectional Self Attention:
the formulae described above
QK go through matmul
MM goes through scale
optional mask from scale
possibly masked scale goes to softmax
and softmax and v end up through one more matmul
"""

# Single head initialization
# each token passed through model as vector
# hidden size defines how wide the token vector is when reaching attention mechanism
# will allow disabling bias terms, but will set on by default


class SingleHeadAttention(nn.Module):
    def __init__(self, hidden_size: int, bias: bool = True):
        super().__init__()
        # in this implementation, start merge Ws into single linear layer
        # unbind inputs into QKV
        # accomplished by increasing out shape by 3
        # equiv to 3 individual lin layers with same in/out shape
        # in multi-head, each head size is smaller than input, so for single head
        # arbitrarily set head size to be 4x smaller than input
        Wqkv = nn.Linear(hidden_size, (hidden_size // 4) * 3, bias=bias)
        # like Wqkv layer, attn layer lin proj
        # layer has optional bias term
        # in single head, projects attended tokens to original shape
        proj = nn.Linear(hidden_size // 4, hidden_sizebias=bias)
