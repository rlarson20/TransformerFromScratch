import math

import torch
import torch.nn as nn
from torch import Tensor, BoolTensor
# from torch.nn import functional as F

"""
Overview of attention mechanism
out: Query Q, Key K and Value V matrices
in: X via linear layers with learnable weights W^Q, W^K and W^V

Q=XW^Q, K=XW^K, V=XW^V

where W^Q is in R^{dim_model x dim_Q}
less formally, Q=XW^{Q} is a set of linear equations
Q=XA^Q + B^Q where A^Q and B^Q are learnable params for calculating Q from X

Attention is calculated by:
Att(Q,K,V) = softmax(QK^T/sqrt(d_k))V

where the denom is a scaling factor usually based on head dimension or num heads
For most implementations, Q,K,V all have the same shape

usually then passed trhough a linear layer W^O projection:
Output = Att(Q,K,V)W^O

essentially a learned weighted avg
attn learns to generate weights btwn tokens via queries and keys
per-token weights generated by softmax
values XW^V learn to create a token repr
which can incorp weighted avg of all other tokens in final dot prod of layer
when token attends to another, its increasing size of second's weight in avg relative to all other tokens
"""

"""
Flavors of attention:
Bidirectional | fully visible
Causal | Autoregressive
Cross

first 2: self-attention, only apply to one input seq
cross applies on multiple inputs

Bi is used in enc blocks in encoder-only models (BERT) or enc-dec models (BART)
allows attn mech to incorp both prior and successive tokens, regardless of order
used when we want context from entire input (classification)

causal: decoder blocks in dec-only (GPT) or enc-dec (BART)
only can work with info from prior tokens
used when trying to preserve temporal structure, like next token generation

cross attn in cross blocks in enc-dec
allows inc a diff seq of tokens to current seq
used when aligning 2 seq: translating from lang/domain to another
or multiple input types, like text and img in diffusion models
"""

"""
Single head self-attention
Simplest mechanism:
Single Head Bidirectional Self Attention:
the formulae described above
QK go through matmul
MM goes through scale
optional mask from scale
possibly masked scale goes to softmax
and softmax and v end up through one more matmul
"""

# Single head initialization
# each token passed through model as vector
# hidden size defines how wide the token vector is when reaching attention mechanism
# will allow disabling bias terms, but will set on by default


class SingleHeadAttention(nn.Module):
    def __init__(self, hidden_size: int, bias: bool = True):
        super().__init__()
        # in this implementation, start merge Ws into single linear layer
        # unbind inputs into QKV
        # accomplished by increasing out shape by 3
        # equiv to 3 individual lin layers with same in/out shape
        # in multi-head, each head size is smaller than input, so for single head
        # arbitrarily set head size to be 4x smaller than input
        self.Wqkv = nn.Linear(hidden_size, (hidden_size // 4) * 3, bias=bias)
        # like Wqkv layer, attn layer lin proj
        # layer has optional bias term
        # in single head, projects attended tokens to original shape
        self.Wo = nn.Linear(hidden_size // 4, hidden_size, bias=bias)

    def forward(self, x: Tensor):
        # single head forward
        # first comp step is to generate QKV.
        # first, pass input x through Wqkv linear
        # reshape output into batch size,
        # one dim for QKV and head size
        # finally, split single tensor into query using unbind
        # batch size, seq len, in dimension
        B, S, C = x.shape
        # split into QKVs of shape B, seq len S, head size HS
        q, k, v = self.Wqkv(x).reshape(B, S, 3, C // 4).unbind(dim=2)
        # now move to math ops of attn
        # first transpose K, take dot of Q and KT
        # calc dot prod of qs and ks of shape
        # (B,S,S) = (B,S,HS) @ (B, HS, S)
        attn = q @ k.transpose(-2, -1)
        # scale by sqrt of head dim
        attn = attn / math.sqrt(k.size(-1))
        # now apply softmax
        attn = attn.softmax(dim=-1)
        # softmax out is how the attn mech weights
        # strength of the relationship btwn pairs of tokens
        # higher softmax val == more importance on that pair of tokens
        # next matmul attn weights w value matrix V, applies attn weights to propagating token embeddings
        x = attn @ v
        # then project to get output
        return self.Wo(x)


"""
Side note on Multi-head self-attn
first, by projecting input to multi
rand-init heads, trans will have multi repr subspaces for same input
giving each trans layer ability to simul
learn diff nuances for same in
second, multi-heads allow attn to jointly attend to multi tokens at same time
even if single wighted is well behaved,
limits ability to focus on multiple
ability to attend to multiple tokens at once

formally, MHA makes 1 QKV per head,
calculates scaled dot prod attn per head
concats attn outputs back into one tensor
before projecting the MHA output through final linear layer
"""

## Multi-head initialization


class BidirectionalAttention(nn.Module):
    def __init__(
        self,
        hidden_size: int,
        num_heads: int,
        attn_drop: float = 0.1,
        out_drop: float = 0.1,
        bias: bool = True,
    ):
        # first need to project to hidden/numheads, so assert they'll work together
        assert hidden_size % num_heads == 0
        # num heads
        self.nh = num_heads
        super().__init__()
        self.Wqkv = nn.Linear(hidden_size, hidden_size * 3, bias=bias)
        self.Wo = nn.Linear(hidden_size, hidden_size, bias=bias)
        # dropout layers to prevent overfitting
        self.attn_drop = nn.Dropout(attn_drop)
        self.out_drop = nn.Dropout(out_drop)

    # forward is mostly the same, few changes to account for multi-heads
    # bi-di attn needs to attend to all tokens in in seq
    # mask exists to support batching diff len seq
    # typically, enc/enc-dec transformer will have
    # pad token, but pad tokens shouldn't interact w sequence tokens
    # this is where mask comes in
    def forward(self, x: Tensor, mask: BoolTensor):
        B, S, C = x.shape
        x = self.Wqkv(x).reshape(B, S, 3, self.nh, C // self.nh)
        q, k, v = x.transpose(3, 1).unbind(dim=2)
        # same mechanism, but difference in tensor shape means we are calculating softmax individually per each head
        # (B, NH, S, S) = (B, NH, S, HS) @ (B, NH, HS, S)
        attn = q @ k.transpose(-2, -1)
        # scale by sqrt of head
        attn = attn / math.sqrt(k.size(-1))

        # reshape and mask attn scores
        attn = attn.masked_fill(mask.view(B, 1, 1, S), float("-inf"))

        # apply softmax for attn weights
        attn = attn.softmax(dim=-1)
        attn = self.attn_drop(attn)

        # last steps are to matmul attn w v, then concat per-head attn into one output of our input
        # done by transposing heads and seqs then reshaping to B,S,C
        # mechanically same as concat, w/o req of making new tensor
        x = attn @ v
        x = x.transpose(1, 2).reshape(B, S, C)
        return self.out_drop(self.Wo(x))


# use upper triang matrix for causal mask
# ensure curr token can only attend to past tokens
#
# create permament causal_mask of shape [context_size,context_size] in init, where ctx_size is max con len of transformer
# to match our padding attn mask, create matrix of bool ones
# then use triu to make upper triang, using diagonal=1 to shift diagonal one to upper-right
# then reshape input to be broadcastable across dims of QK^T, which is B, NH, S, S and assign to
# pytorch buffer
class CausalAttention(nn.Module):
    def __init__(
        self,
        hidden_size: int,
        num_heads: int,
        context_size: int,
        attn_drop: float = 0.1,
        out_drop: float = 0.1,
        bias: bool = True,
    ):
        assert hidden_size % num_heads == 0
        self.nh = num_heads
        super().__init__()
        self.Wqkv = nn.Linear(hidden_size, hidden_size * 3, bias=bias)
        self.Wo = nn.Linear(hidden_size, hidden_size, bias=bias)
        self.attn_drop = nn.Dropout(attn_drop)
        self.out_drop = nn.Dropout(out_drop)

        self.register_buffer(
            "causal_mask",
            torch.triu(
                torch.ones([context_size, context_size], dtype=torch.bool), diagonal=1
            ).view(1, 1, context_size, context_size),
        )

    def forward(self, x: Tensor, mask: BoolTensor):
        B, S, C = x.shape

        x = self.Wqkv(x).reshape(B, S, 3, self.nh, C // self.nh)
        q, k, v = x.transpose(3, 1).unbind(dim=2)

        attn = q @ k.transpose(-2, -1)
        attn = attn / math.sqrt(k.size(-1))

        combined_mask = self.causal_mask[:, :, :S, :S]
        if mask is not None:
            combined_mask = combined_mask | mask.view(B, 1, 1, S)
        attn = attn.masked_fill(combined_mask, float("-inf"))

        attn = attn.softmax(dim=-1)
        attn = self.attn_drop(attn)

        x = attn @ v

        x = x.transpose(1, 2).reshape(B, S, C)
        return self.out_drop(self.Wo(x))


"""
All versions of attn have been self-attention
cross attention applies across input sequences

formal def needs some modifications
author of articles follows original cross attn
where the query Q is made from first seq X and both keys and vals from second seq Y

Not set in stone
other transformer models adopt diff allocations of the first and second sequence across the QKV

Formal defn is same, but
Q_X, K_Y, V_Y representing MH Qs, Ks, and Vs for inputs x and y respectively

Q_X = XW_{h}^{Q}
K_Y = YW_{h}^{K}
V_Y = YW_{h}^{V}

CrossAttn(Q_X,K_Y,V_Y) = softmax(Q_X K_Y^T/sqrt(d_h))V_Y

output = CrossAttn(Q_X,K_Y,V_Y)W^O

requires separate lin layers for qs and kvs

other than that, assuming this is in decoder side of enc-dec trans, everything after making QKV is same as causal attn
if making it for encoder style attn layer,
would remove causal mask and keep input mask if appropriate
"""


class CausalCrossAttention(nn.Module):
    def __init__(
        self,
        hidden_size: int,
        num_heads: int,
        context_size: int,
        attn_drop: float = 0.1,
        out_drop: float = 0.1,
        bias: bool = True,
    ):
        super().__init__()
        assert hidden_size % num_heads == 0
        self.nh = num_heads
        self.Wq = nn.Linear(hidden_size, hidden_size, bias=bias)
        self.Wkv = nn.Linear(hidden_size, hidden_size * 2, bias=bias)
        self.Wo = nn.Linear(hidden_size, hidden_size, bias=bias)
        self.attn_drop = nn.Dropout(attn_drop)
        self.out_drop = nn.Dropout(out_drop)
        self.register_buffer(
            "causal_mask",
            torch.triu(
                torch.ones([context_size, context_size], dtype=torch.bool), diagonal=1
            ).view(1, 1, context_size, context_size),
        )

    def forward(self, x: Tensor, y: Tensor, mask: BoolTensor):
        B, S, C = x.shape

        # split into qs of shape B,NH,S,HS from decoder input
        q = self.Wq(x).reshape(B, S, C // self.nh).transpose(1, 2)

        # split into keys and vals of shape (B,NH,S,HS) from encoder output
        y = self.Wky(y).reshape(B, S, 2, self.nh, C // self.nh)
        k, v = y.transpose(3, 1).unbind(dim=2)

        attn = q @ k.transpose(-2, -1)
        attn = attn / math.sqrt(k.size(-1))

        combined_mask = self.causal_mask + mask.view(B, 1, 1, S)
        attn = attn.masked_fill(combined_mask, float("-inf"))

        attn = attn.softmax(dim=-1)
        attn = self.attn_drop(attn)

        x = attn @ v

        x = x.transpose(1, 2).reshape(B, S, C)
        return self.out_drop(self.Wo(x))
