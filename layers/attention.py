import math

import torch.nn as nn
# from torch import Tensor, BoolTensor
# from torch.nn import functional as F

"""
Overview of attention mechanism
out: Query Q, Key K and Value V matrices
in: X via linear layers with learnable weights W^Q, W^K and W^V

Q=XW^Q, K=XW^K, V=XW^V

where W^Q is in R^{dim_model x dim_Q}
less formally, Q=XW^{Q} is a set of linear equations
Q=XA^Q + B^Q where A^Q and B^Q are learnable params for calculating Q from X

Attention is calculated by:
Att(Q,K,V) = softmax(QK^T/sqrt(d_k))V

where the denom is a scaling factor usually based on head dimension or num heads
For most implementations, Q,K,V all have the same shape

usually then passed trhough a linear layer W^O projection:
Output = Att(Q,K,V)W^O

essentially a learned weighted avg
attn learns to generate weights btwn tokens via queries and keys
per-token weights generated by softmax
values XW^V learn to create a token repr
which can incorp weighted avg of all other tokens in final dot prod of layer
when token attends to another, its increasing size of second's weight in avg relative to all other tokens
"""

"""
Flavors of attention:
Bidirectional | fully visible
Causal | Autoregressive
Cross

first 2: self-attention, only apply to one input seq
cross applies on multiple inputs

Bi is used in enc blocks in encoder-only models (BERT) or enc-dec models (BART)
allows attn mech to incorp both prior and successive tokens, regardless of order
used when we want context from entire input (classification)

causal: decoder blocks in dec-only (GPT) or enc-dec (BART)
only can work with info from prior tokens
used when trying to preserve temporal structure, like next token generation

cross attn in cross blocks in enc-dec
allows inc a diff seq of tokens to current seq
used when aligning 2 seq: translating from lang/domain to another
or multiple input types, like text and img in diffusion models
"""

"""
Single head self-attention
Simplest mechanism:
Single Head Bidirectional Self Attention:
the formulae described above
QK go through matmul
MM goes through scale
optional mask from scale
possibly masked scale goes to softmax
and softmax and v end up through one more matmul
"""

# Single head initialization
# each token passed through model as vector
# hidden size defines how wide the token vector is when reaching attention mechanism
# will allow disabling bias terms, but will set on by default


class SingleHeadAttention(nn.Module):
    def __init__(self, hidden_size: int, bias: bool = True):
        super().__init__()
        # in this implementation, start merge Ws into single linear layer
        # unbind inputs into QKV
        # accomplished by increasing out shape by 3
        # equiv to 3 individual lin layers with same in/out shape
        # in multi-head, each head size is smaller than input, so for single head
        # arbitrarily set head size to be 4x smaller than input
        self.Wqkv = nn.Linear(hidden_size, (hidden_size // 4) * 3, bias=bias)
        # like Wqkv layer, attn layer lin proj
        # layer has optional bias term
        # in single head, projects attended tokens to original shape
        self.Wo = nn.Linear(hidden_size // 4, hidden_sizebias=bias)

    def forward(self, x):
        # single head forward
        # first comp step is to generate QKV.
        # first, pass input x through Wqkv linear
        # reshape output into batch size,
        # one dim for QKV and head size
        # finally, split single tensor into query using unbind
        # batch size, seq len, in dimension
        B, S, C = x.shape
        # split into QKVs of shape B, seq len S, head size HS
        q, k, v = self.Wqkv(x).reshape(B, S, 3, C // 4).unbind(dim=2)
        # now move to math ops of attn
        # first transpose K, take dot of Q and KT
        # calc dot prod of qs and ks of shape
        # (B,S,S) = (B,S,HS) @ (B, HS, S)
        attn = q @ k.transpose(-2, -1)
        # scale by sqrt of head dim
        attn = attn / math.sqrt(k.size(-1))
        # now apply softmax
        attn = attn.softmax(dim=-1)
        # softmax out is how the attn mech weights
        # strength of the relationship btwn pairs of tokens
        # higher softmax val == more importance on that pair of tokens
        # next matmul attn weights w value matrix V, applies attn weights to propagating token embeddings
        x = attn @ v
        # then project to get output
        return self.Wo(x)


"""
Side note on Multi-head self-attn
first, by projecting input to multi
rand-init heads, trans will have multi repr subspaces for same input
giving each trans layer ability to simul
learn diff nuances for same in
second, multi-heads allow attn to jointly attend to multi tokens at same time
even if single wighted is well behaved,
limits ability to focus on multiple
ability to attend to multiple tokens at once

formally, MHA makes 1 QKV per head,
calculates scaled dot prod attn per head
concats attn outputs back into one tensor
before projecting the MHA output through final linear layer
"""

## Multi-head initialization


class MultiHeadAttention(nn.Module):
    def __init__(self, hidden_size: int, num_heads: int, bias: bool = True):
        # first need to project to hidden/numheads, so assert they'll work together
        assert hidden_size % num_heads == 0
        # num heads
        self.nh = num_heads
        super().__init__()
        self.Wqkv = nn.Linear(hidden_size, hidden_size * 3, bias=bias)
        self.Wo = nn.Linear(hidden_size, hidden_size, bias=bias)

    # forward is mostly the same, few changes to account for multi-heads
    def forward(self, x):
        B, S, C = x.shape
        x = self.Wqkv(x).reshape(B, S, 3, self.nh, C // self.nh)
        q, k, v = x.transpose(3, 1).unbind(dim=2)
        # same mechanism, but difference in tensor shape means we are calculating softmax individually per each head
        # (B, NH, S, S) = (B, NH, S, HS) @ (B, NH, HS, S)
        attn = q @ k.transpose(-2, -1)
        attn = attn / math.sqrt(k.size(-1))
        attn = attn.softmax(dim=-1)

        # last steps are to matmul attn w v, then concat per-head attn into one output of our input
        # done by transposing heads and seqs then reshaping to B,S,C
        # mechanically same as concat, w/o req of making new tensor
        x = attn @ v
        x = x.transpose(1, 2).reshape(B, S, C)
        return self.Wo(x)
